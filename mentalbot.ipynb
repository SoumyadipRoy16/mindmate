{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8725559,"sourceType":"datasetVersion","datasetId":5236464},{"sourceId":8725583,"sourceType":"datasetVersion","datasetId":5236481}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import json\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Load the JSON data\nwith open('/kaggle/input/mental-health-bot/KB.json', 'r') as file:\n    data = json.load(file)\n\n# Create lists to hold the patterns and corresponding tags\npatterns = []\ntags = []\nresponses = {}\n\n# Extract patterns and tags from the JSON data\nfor intent in data['intents']:\n    for pattern in intent['patterns']:\n        patterns.append(pattern)\n        tags.append(intent['tag'])\n    responses[intent['tag']] = ['responses']\n\n\n# Create a DataFrame from the lists\ndf = pd.DataFrame({\n    'text': patterns,\n    'label': tags\n})\n\n# Split the data into training and validation sets\ntrain_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n\n# Save the training and validation sets to CSV files\ntrain_df.to_csv('train.csv', index=False)\nval_df.to_csv('val.csv', index=False)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-21T11:02:09.108208Z","iopub.execute_input":"2024-06-21T11:02:09.108581Z","iopub.status.idle":"2024-06-21T11:02:12.124297Z","shell.execute_reply.started":"2024-06-21T11:02:09.108549Z","shell.execute_reply":"2024-06-21T11:02:12.123034Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\napi_key = '58966ed76bae78a37398734ec44654fab54104bd'\n\n# Load the tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n# Tokenize the training data\ntrain_encodings = tokenizer(train_df['text'].tolist(), truncation=True, padding=True)\nval_encodings = tokenizer(val_df['text'].tolist(), truncation=True, padding=True)\n\n# Convert labels to numerical values\nlabel2id = {label: idx for idx, label in enumerate(df['label'].unique())}\nid2label = {idx: label for label, idx in label2id.items()}\ntrain_labels = train_df['label'].map(label2id).tolist()\nval_labels = val_df['label'].map(label2id).tolist()\n\n# Create a Dataset class\nclass IntentDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        return item\n\n# Create dataset objects\ntrain_dataset = IntentDataset(train_encodings, train_labels)\nval_dataset = IntentDataset(val_encodings, val_labels)\n\n# Load the BERT model\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(label2id))\n\n# Define training arguments\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    num_train_epochs=3,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=16,\n    warmup_steps=500,\n    weight_decay=0.01,\n    logging_dir='./logs',\n    logging_steps=10,\n    evaluation_strategy=\"epoch\"\n)\n\n# Create a Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset\n)\n\n# Train the model\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-06-21T11:02:12.126961Z","iopub.execute_input":"2024-06-21T11:02:12.127346Z","iopub.status.idle":"2024-06-21T12:20:15.851138Z","shell.execute_reply.started":"2024-06-21T11:02:12.127313Z","shell.execute_reply":"2024-06-21T12:20:15.849737Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-06-21 11:02:21.895121: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-21 11:02:21.895282: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-21 11:02:22.084867: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"22d34e9ec7e34ee29340089441ff605c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7dda55b759f3406bb2422be1b83a45a8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ff355e5e25b431081f6234f2b26865e"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d62f100a288b428dad2a89c46321ec53"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"394b8fc4d664411a9da16d9cec7efaec"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.2 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240621_111305-15pgqhlu</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/mindmate/huggingface/runs/15pgqhlu' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/mindmate/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/mindmate/huggingface' target=\"_blank\">https://wandb.ai/mindmate/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/mindmate/huggingface/runs/15pgqhlu' target=\"_blank\">https://wandb.ai/mindmate/huggingface/runs/15pgqhlu</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1878' max='1878' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1878/1878 1:06:49, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.544400</td>\n      <td>0.491771</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.081600</td>\n      <td>0.233004</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.113700</td>\n      <td>0.204191</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1878, training_loss=0.7600456657834327, metrics={'train_runtime': 4652.5953, 'train_samples_per_second': 3.227, 'train_steps_per_second': 0.404, 'total_flos': 254756735114112.0, 'train_loss': 0.7600456657834327, 'epoch': 3.0})"},"metadata":{}}]},{"cell_type":"code","source":"# Save the model and tokenizer\nmodel.save_pretrained('bert-mental-health-model')\ntokenizer.save_pretrained('bert-mental-health-tokenizer')","metadata":{"execution":{"iopub.status.busy":"2024-06-21T12:20:15.853212Z","iopub.execute_input":"2024-06-21T12:20:15.853704Z","iopub.status.idle":"2024-06-21T12:20:16.503084Z","shell.execute_reply.started":"2024-06-21T12:20:15.853654Z","shell.execute_reply":"2024-06-21T12:20:16.501611Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"('bert-mental-health-tokenizer/tokenizer_config.json',\n 'bert-mental-health-tokenizer/special_tokens_map.json',\n 'bert-mental-health-tokenizer/vocab.txt',\n 'bert-mental-health-tokenizer/added_tokens.json')"},"metadata":{}}]},{"cell_type":"code","source":"# Evaluate the model\neval_result = trainer.evaluate()\n\n# Print the evaluation results\nprint(f\"Evaluation results: {eval_result}\")","metadata":{"execution":{"iopub.status.busy":"2024-06-21T12:20:16.504937Z","iopub.execute_input":"2024-06-21T12:20:16.505307Z","iopub.status.idle":"2024-06-21T12:21:16.947331Z","shell.execute_reply.started":"2024-06-21T12:20:16.505275Z","shell.execute_reply":"2024-06-21T12:21:16.946025Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='79' max='79' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [79/79 00:59]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Evaluation results: {'eval_loss': 0.20419113337993622, 'eval_runtime': 60.4304, 'eval_samples_per_second': 20.701, 'eval_steps_per_second': 1.307, 'epoch': 3.0}\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom transformers import BertTokenizer, BertForSequenceClassification\n\n# Load the fine-tuned model and tokenizer\nmodel = BertForSequenceClassification.from_pretrained('bert-mental-health-model')\ntokenizer = BertTokenizer.from_pretrained('bert-mental-health-tokenizer')\n\n# Function to predict the intent\ndef predict_intent(text):\n    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n    outputs = model(**inputs)\n    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n    predicted_class = predictions.argmax().item()\n    return id2label[predicted_class]\n\n# Test the function\ntext = \"Hi, I need help with my anxiety.\"\nintent = predict_intent(text)\nprint(f\"Predicted intent: {intent}\")","metadata":{"execution":{"iopub.status.busy":"2024-06-21T12:21:16.951052Z","iopub.execute_input":"2024-06-21T12:21:16.951494Z","iopub.status.idle":"2024-06-21T12:21:17.477599Z","shell.execute_reply.started":"2024-06-21T12:21:16.951460Z","shell.execute_reply":"2024-06-21T12:21:17.476509Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Predicted intent: anxious\n","output_type":"stream"}]},{"cell_type":"code","source":"import json\n\n# Load the JSON data\nwith open('/kaggle/input/mental-health-bot/KB.json', 'r') as file:\n    data = json.load(file)\n\n# Create dictionaries to hold the patterns, tags, and responses\npatterns = []\ntags = []\nresponses = {}\n\n# Extract patterns, tags, and responses from the JSON data\nfor intent in data['intents']:\n    for pattern in intent['patterns']:\n        patterns.append(pattern)\n        tags.append(intent['tag'])\n    responses[intent['tag']] = ['responses']\n\n# Create a DataFrame from the lists\ndf = pd.DataFrame({\n    'text': patterns,\n    'label': tags\n})\n\n# Split the data into training and validation sets\nfrom sklearn.model_selection import train_test_split\ntrain_df, val_df = train_test_split(df, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-06-21T12:21:17.479033Z","iopub.execute_input":"2024-06-21T12:21:17.479863Z","iopub.status.idle":"2024-06-21T12:21:17.537022Z","shell.execute_reply.started":"2024-06-21T12:21:17.479824Z","shell.execute_reply":"2024-06-21T12:21:17.535696Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import BertTokenizer, BertForSequenceClassification\nimport random\n\n# Load the fine-tuned model and tokenizer\nmodel = BertForSequenceClassification.from_pretrained('/kaggle/working/bert-mental-health-model')\ntokenizer = BertTokenizer.from_pretrained('/kaggle/working/bert-mental-health-tokenizer')\n\n# Function to predict the intent and generate a response\ndef predict_intent_and_respond(text):\n    # Predict the intent\n    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n    outputs = model(**inputs)\n    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n    predicted_class = predictions.argmax().item()\n    intent = id2label[predicted_class]\n    \n    # Generate a response\n    response = random.choice(responses[intent])\n    return response\n\n# Test the function\ntext = \"Hi, I need help with my anxiety.\"\nresponse = predict_intent_and_respond(text)\nprint(f\"Response: {response}\")","metadata":{"execution":{"iopub.status.busy":"2024-06-21T12:21:17.539136Z","iopub.execute_input":"2024-06-21T12:21:17.539511Z","iopub.status.idle":"2024-06-21T12:21:18.203839Z","shell.execute_reply.started":"2024-06-21T12:21:17.539478Z","shell.execute_reply":"2024-06-21T12:21:18.202897Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Response: responses\n","output_type":"stream"}]}]}